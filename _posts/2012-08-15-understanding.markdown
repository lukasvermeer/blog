---
layout: post
title: "Understanding"
date: 2012-08-15
tags: ["Artificial Intelligence","black box","Datamining","models","problem solving","Statistics"]
original:
  source_name: Wordpress
  source_url: https://lukasvermeer.wordpress.com/2012/08/15/understanding/
---

Derek Jones posits that "[success does not require understanding](http://shape-of-code.coding-guidelines.com/2012/07/23/success-does-not-require-understanding/)".
> In my line of work I am constantly trying to understand what is going on (the purpose of this understanding is to control and make things better) and consider anybody who uses machine learning as being clueless, dim witted or just plain lazy; the problem with machine learning is that it gives answers without explanations (ok decision trees do provide some insights).
**Problem solving versus solving problems.**

As one who specializes in using machine learning, I obviously resent being called "clueless, dim witted or just plain lazy". However, I feel a larger point should be made here. Success _does_ most definitely require understanding, but not necessarily of how one particular instance of a solution came about.

To be successful in any machine learning effort, one needs to have intricate understanding of what the problem is and how techniques can be applied to find solutions. This is a more general form of understanding which puts more emphasis on the process of finding workable models, rather than on applying these models to individual instances of a problem. Comprehension of problem solving over understanding a particular solution.

**Driving a black box.**

Consider the following example. To me, the engine of my car is a black box; I have very little idea how it works. My mechanic does know how engines work in general, but he is unable to know the exact internal state of the engine in my car as I am cruising down the highway at 100 miles per hour. None of this "lack of understanding" prevents me from getting from A to B. I turn the wheel, I push the peddel and off we go.

In essence, my mechanic and I have different levels of understanding of my car. But importantly, at different levels of precision, the thing becomes a black box to each of us; in the sense that there is a point where our otherwise perfectly practical models break down and no longer are able to reflect reality. In the end, it's black boxes [all the way down](http://en.wikipedia.org/wiki/Turtles_all_the_way_down).

**[Chasing shadows](http://en.wikipedia.org/wiki/Allegory_of_the_Cave).**

Models are merely tools to help you navigate a vastly complex world. Very much like machine learning models, a scientific model might work in many cases, but so does [Newton's law of universal gravitation](http://en.wikipedia.org/wiki/Newton). We know for a fact that that particular model is definitely wrong; and I [sincerely hope many others are just as incorrect](https://twitter.com/zachweiner/status/235031757156667393).

There will always be limits to our understanding. The fact that we have a model that can help us predict does not necessarily mean we have correctly understood the nature of the universe. [All models are wrong, but some are useful](http://www.wired.com/science/discoveries/magazine/16-07/pb_theory).

Reality is simply much too complicated to be captured in a manageable set of rules, but even incomplete (or incorrect) models can provide insight and help us better navigate this world. Machine learning is successful, precisely because it can help us find such models.

<span style="color:#c0c0c0;">[ Peter Norvig has written an [<span style="color:#c0c0c0;">excellent piece</span>](http://norvig.com/chomsky.html) on this subject in relation to language models. ]</span>